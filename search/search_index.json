{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hello there! This is my cv! (minus stopwords) Here you can find some notes, snippets, reflections, summaries, and general thoughts on what I find interesting to save on my daily basis. Topics are usually related to one of: Artificial Intelligence, Data Science, Machine Learning, Natural Language Processing, Computer Vision, and friends. Python in general. CI, Devops. Iot Devices. Embedded computing. Mechanical Keyboards. gists These are quickreads ( <10[min] ) which show something very specific which I found interesting. Most of these are here because they belong to one of the following categories: Small tweaks aimed at boosting unix, docker, or python-related productivity. Interesting fixes, hacks, or workaround I've had to use in the past. Funny or intriguing tutorials I expect to save time redirecting people here (: projects These are somewhat more involved, and might require some previous background to understand / make sense of.","title":"Hello there!"},{"location":"#hello-there","text":"This is my cv! (minus stopwords) Here you can find some notes, snippets, reflections, summaries, and general thoughts on what I find interesting to save on my daily basis. Topics are usually related to one of: Artificial Intelligence, Data Science, Machine Learning, Natural Language Processing, Computer Vision, and friends. Python in general. CI, Devops. Iot Devices. Embedded computing. Mechanical Keyboards.","title":"Hello there!"},{"location":"#gists","text":"These are quickreads ( <10[min] ) which show something very specific which I found interesting. Most of these are here because they belong to one of the following categories: Small tweaks aimed at boosting unix, docker, or python-related productivity. Interesting fixes, hacks, or workaround I've had to use in the past. Funny or intriguing","title":"gists"},{"location":"#tutorials","text":"I expect to save time redirecting people here (:","title":"tutorials"},{"location":"#projects","text":"These are somewhat more involved, and might require some previous background to understand / make sense of.","title":"projects"},{"location":"gists/2020-10-07-tty-browser/","text":"Tldr To browse the web from a TTY (!), with docker installed: curl -L git.io/netsurf | python3 netsurf -w 1920 -h 1080 google.com/search?q=\"terry tao\" Why Two reasons: I've been, for some time now, interested on not depending on x11 In remote debugging session, or in general in abscence of X server (not by choice), I end up needing google something. While it is true you can just use your mobile, another machine, or text based browsers like lynx, none of these hit the tradeoff between speed and interactivity (not with me behind the keyboard) This was the perfect excuse to play a little bit with the framebuffer: Turns out, with appropiate permisions (eg being in the video group), you can directly \"draw\" pixels on the screen cat /dev/urandom > /dev/fb0 (Here, the framebuffer is located at /dev/fb0 ) How The netsurf browser has several drawing backends. One of them is (SDL) framebuffer. The challenge here was to find out the compile and runtime reqs and params to make it work. Once the correct parameters, configurations and commands are correct, you can run netsurf to surf the net from the tty, without X server. The results can be found in this dockerfile","title":"Browsing the web from the terminal"},{"location":"gists/2020-10-07-tty-browser/#why","text":"Two reasons: I've been, for some time now, interested on not depending on x11 In remote debugging session, or in general in abscence of X server (not by choice), I end up needing google something. While it is true you can just use your mobile, another machine, or text based browsers like lynx, none of these hit the tradeoff between speed and interactivity (not with me behind the keyboard) This was the perfect excuse to play a little bit with the framebuffer: Turns out, with appropiate permisions (eg being in the video group), you can directly \"draw\" pixels on the screen cat /dev/urandom > /dev/fb0 (Here, the framebuffer is located at /dev/fb0 )","title":"Why"},{"location":"gists/2020-10-07-tty-browser/#how","text":"The netsurf browser has several drawing backends. One of them is (SDL) framebuffer. The challenge here was to find out the compile and runtime reqs and params to make it work. Once the correct parameters, configurations and commands are correct, you can run netsurf to surf the net from the tty, without X server. The results can be found in this dockerfile","title":"How"},{"location":"gists/avoiding-sleep/","text":"TL;DR I had some issues working with wipy ( picom micropython ) for a multithreaded deployment. Specifically, I wanted the following: Have the thread worker return its value to the main thread (this is pretty commmon, so there's a known patten for that). Solve an issue with the library where you had to manually sleep in the main thread, or it would block the auxiliary threads. The problem See the original description of the problem and the proposed workaround here: pycom forums The solution Insights: If the main thread does not allow the other threads to run, it means it is not waining enough for \"something\" to happen. Evidence of this is that manually sleeping in the main thread is the recommended workaround. As the code interacted with the _thread module mainly in the _thread.start_new_thread , there were two possible delays: The time it takes to run the _thread.start_new_thread instruction The time it takes from then until the target callback is called. I first tried using a lock around the instruction (case 1), which did not solve the problem. Finally, I implemented a solution where use an external lock before the _thread.start_new_thread instruction, and release it at the beginning of the target call. Sample implementation This is a demo for the code I ended up using. import time import _thread class Thread : def init ( self , target ): self . result = None self . target = target self . lock = _thread . allocate_lock () self . aux_lock = _thread . allocate_lock () def real_call ( self ): with self . lock : self . aux_lock . release () self . result = self . target () def call ( self ): self . aux_lock . acquire () _thread . start_new_thread ( self . real_call , ()) with self . aux_lock : pass def join ( self ): with self . lock : pass def demo (): print ( \"Starting\" ) time . sleep ( 10 ) print ( \"Ending\" ) return 42 def main (): t = Thread ( target = demo ) t () t . join () print ( \"Result: \" + str ( t . result )) if name == \"main\" : main ()","title":"A workaround story with double locks"},{"location":"gists/avoiding-sleep/#tldr","text":"I had some issues working with wipy ( picom micropython ) for a multithreaded deployment. Specifically, I wanted the following: Have the thread worker return its value to the main thread (this is pretty commmon, so there's a known patten for that). Solve an issue with the library where you had to manually sleep in the main thread, or it would block the auxiliary threads. The problem See the original description of the problem and the proposed workaround here: pycom forums","title":"TL;DR"},{"location":"gists/avoiding-sleep/#the-solution","text":"Insights: If the main thread does not allow the other threads to run, it means it is not waining enough for \"something\" to happen. Evidence of this is that manually sleeping in the main thread is the recommended workaround. As the code interacted with the _thread module mainly in the _thread.start_new_thread , there were two possible delays: The time it takes to run the _thread.start_new_thread instruction The time it takes from then until the target callback is called. I first tried using a lock around the instruction (case 1), which did not solve the problem. Finally, I implemented a solution where use an external lock before the _thread.start_new_thread instruction, and release it at the beginning of the target call.","title":"The solution"},{"location":"gists/avoiding-sleep/#sample-implementation","text":"This is a demo for the code I ended up using. import time import _thread class Thread : def init ( self , target ): self . result = None self . target = target self . lock = _thread . allocate_lock () self . aux_lock = _thread . allocate_lock () def real_call ( self ): with self . lock : self . aux_lock . release () self . result = self . target () def call ( self ): self . aux_lock . acquire () _thread . start_new_thread ( self . real_call , ()) with self . aux_lock : pass def join ( self ): with self . lock : pass def demo (): print ( \"Starting\" ) time . sleep ( 10 ) print ( \"Ending\" ) return 42 def main (): t = Thread ( target = demo ) t () t . join () print ( \"Result: \" + str ( t . result )) if name == \"main\" : main ()","title":"Sample implementation"},{"location":"gists/batch-delete-workflow-runs/","text":"When adapting or updating github actions workflows, something is bound to break. Because of this, you'll end up with a bunch of runs which couldd contain sensitive / grabage information. Thanks to this guy here , here's the solution: Download and install the github cli app - gh Add permissions: gh auth login Run the following snippet: owner=GITHUB_REPO_OWNER repo=GITHUB_REPO_NAME gh api \\ repos/ \\(owner/\\) repo/actions/runs \\ | jq -r '.workflow_runs[] | select(.head_branch != \"master\") | \"(.id)\"' \\ | xargs -n1 -I % gh api repos/ \\(owner/\\) repo/actions/runs/% -X DELETE Successfully installed blackini cat tox.ini [tool.black] line-length = 79 target-version = py36, black --config-file=tox.ini ... You might need to run it several times, as it can delete a bunch before exiting","title":"Deleting several github actions workflow runs at once"},{"location":"gists/blackini/","text":"Tldr Just install with pip pip install blackini Successfully installed blackini cat tox.ini [tool.black] line-length = 79 target-version = py36, black --config-file=tox.ini ... Deprecation Notice Although the project is fully functional, I no longer use this method as the rest of the python ecosystem has evolved and now most tools accept configuration from pyproject.toml . Motivation This is a black wrapper to allow it to read configuration from .ini files. The developer toolbox for python includes formatters, linters, and testing libraries. In my case, this means (at least): black, flake8, pytest and tox. For code formatting, although black demands no configuration, you can tweak some minor settings (mainly max-line-length, in my case) by either of two options: calling black with command-line args, or by defining a [tool.black] table in a pyproject.toml file. While the former rapidly becomes tedious and error-prone, the latter also has its own drawbacks, as most other tools (flake8, mypy, isort, bandit, tox, to name a few) can be configured with a single tox.ini... Enter blackini, \"A black wrapper to read config from .ini files.\" How does it work? By changing just two lines in the read_pyproject function, we can make black load configuration from another source (or format): that's exactly what blackini does. Create a patched configfile loader Basically, just read the .ini file as dict , and return the same structure toml.decoder.load would. If anything fails, use normal toml mode for a graceful fallback. #!/usr/bin/env python # -*- coding: utf-8 -*- \"\"\"Monkeypatch black to allow reding config from '.ini` files.\"\"\" import configparser import re import sys import toml from black import main as main_black def patched_load ( * a , ** kw ): \"\"\"Attempt to read configuration from .ini, if it makes sense.\"\"\" try : from pathlib import Path file_ = a [ 0 ] if Path ( file_ ) . suffix == \".ini\" : config = configparser . ConfigParser () config . read ( file_ ) return { \"tool\" : { \"black\" : { k : v if \",\" not in v else [ p for p in v . split ( \",\" ) if p ] for k , v in config [ \"tool.black\" ] . items () } } } except BaseException : # pylint: disable=W0703 pass return toml . decoder . load ( * a , ** kw ) def main (): \"\"\"Monkey-patch `toml.load` before calling black.\"\"\" sys . argv [ 0 ] = re . sub ( r \"(-script\\.pyw?|\\.exe)?$\" , \"\" , sys . argv [ 0 ]) toml . load = patched_load sys . exit ( main_black ()) if __name__ == \"__main__\" : main () Patch toml loader to allow .ini Next, we make sure that black uses this version instead of the original one by monkeypatching toml.load : #!/usr/bin/env python # -*- coding: utf-8 -*- \"\"\"Monkeypatch black to allow reding config from '.ini` files.\"\"\" import configparser import re import sys import toml from black import main as main_black def patched_load ( * a , ** kw ): \"\"\"Attempt to read configuration from .ini, if it makes sense.\"\"\" try : from pathlib import Path file_ = a [ 0 ] if Path ( file_ ) . suffix == \".ini\" : config = configparser . ConfigParser () config . read ( file_ ) return { \"tool\" : { \"black\" : { k : v if \",\" not in v else [ p for p in v . split ( \",\" ) if p ] for k , v in config [ \"tool.black\" ] . items () } } } except BaseException : # pylint: disable=W0703 pass return toml . decoder . load ( * a , ** kw ) def main (): \"\"\"Monkey-patch `toml.load` before calling black.\"\"\" sys . argv [ 0 ] = re . sub ( r \"(-script\\.pyw?|\\.exe)?$\" , \"\" , sys . argv [ 0 ]) toml . load = patched_load sys . exit ( main_black ()) if __name__ == \"__main__\" : main () Call black 's main Finally, just call black: #!/usr/bin/env python # -*- coding: utf-8 -*- \"\"\"Monkeypatch black to allow reding config from '.ini` files.\"\"\" import configparser import re import sys import toml from black import main as main_black def patched_load ( * a , ** kw ): \"\"\"Attempt to read configuration from .ini, if it makes sense.\"\"\" try : from pathlib import Path file_ = a [ 0 ] if Path ( file_ ) . suffix == \".ini\" : config = configparser . ConfigParser () config . read ( file_ ) return { \"tool\" : { \"black\" : { k : v if \",\" not in v else [ p for p in v . split ( \",\" ) if p ] for k , v in config [ \"tool.black\" ] . items () } } } except BaseException : # pylint: disable=W0703 pass return toml . decoder . load ( * a , ** kw ) def main (): \"\"\"Monkey-patch `toml.load` before calling black.\"\"\" sys . argv [ 0 ] = re . sub ( r \"(-script\\.pyw?|\\.exe)?$\" , \"\" , sys . argv [ 0 ]) toml . load = patched_load sys . exit ( main_black ()) if __name__ == \"__main__\" : main () Et voil\u00e0, you have a black callable which loads config form tox.ini. Closing blackini is available on PyPI here , and it has a loose dependency on black , so you can just pip install blackini (or pin a specific black version, by installing it before blackini ). Once installed, it will overwrite the black executable. This is enough for my most common use case scenario, and allows to move the black configuration inside the same tox.ini file, just like the rest of the tools.","title":"Blackini"},{"location":"gists/blackini/#motivation","text":"This is a black wrapper to allow it to read configuration from .ini files. The developer toolbox for python includes formatters, linters, and testing libraries. In my case, this means (at least): black, flake8, pytest and tox. For code formatting, although black demands no configuration, you can tweak some minor settings (mainly max-line-length, in my case) by either of two options: calling black with command-line args, or by defining a [tool.black] table in a pyproject.toml file. While the former rapidly becomes tedious and error-prone, the latter also has its own drawbacks, as most other tools (flake8, mypy, isort, bandit, tox, to name a few) can be configured with a single tox.ini... Enter blackini, \"A black wrapper to read config from .ini files.\"","title":"Motivation"},{"location":"gists/blackini/#how-does-it-work","text":"By changing just two lines in the read_pyproject function, we can make black load configuration from another source (or format): that's exactly what blackini does.","title":"How does it work?"},{"location":"gists/blackini/#create-a-patched-configfile-loader","text":"Basically, just read the .ini file as dict , and return the same structure toml.decoder.load would. If anything fails, use normal toml mode for a graceful fallback. #!/usr/bin/env python # -*- coding: utf-8 -*- \"\"\"Monkeypatch black to allow reding config from '.ini` files.\"\"\" import configparser import re import sys import toml from black import main as main_black def patched_load ( * a , ** kw ): \"\"\"Attempt to read configuration from .ini, if it makes sense.\"\"\" try : from pathlib import Path file_ = a [ 0 ] if Path ( file_ ) . suffix == \".ini\" : config = configparser . ConfigParser () config . read ( file_ ) return { \"tool\" : { \"black\" : { k : v if \",\" not in v else [ p for p in v . split ( \",\" ) if p ] for k , v in config [ \"tool.black\" ] . items () } } } except BaseException : # pylint: disable=W0703 pass return toml . decoder . load ( * a , ** kw ) def main (): \"\"\"Monkey-patch `toml.load` before calling black.\"\"\" sys . argv [ 0 ] = re . sub ( r \"(-script\\.pyw?|\\.exe)?$\" , \"\" , sys . argv [ 0 ]) toml . load = patched_load sys . exit ( main_black ()) if __name__ == \"__main__\" : main ()","title":"Create a patched configfile loader"},{"location":"gists/blackini/#patch-toml-loader-to-allow-ini","text":"Next, we make sure that black uses this version instead of the original one by monkeypatching toml.load : #!/usr/bin/env python # -*- coding: utf-8 -*- \"\"\"Monkeypatch black to allow reding config from '.ini` files.\"\"\" import configparser import re import sys import toml from black import main as main_black def patched_load ( * a , ** kw ): \"\"\"Attempt to read configuration from .ini, if it makes sense.\"\"\" try : from pathlib import Path file_ = a [ 0 ] if Path ( file_ ) . suffix == \".ini\" : config = configparser . ConfigParser () config . read ( file_ ) return { \"tool\" : { \"black\" : { k : v if \",\" not in v else [ p for p in v . split ( \",\" ) if p ] for k , v in config [ \"tool.black\" ] . items () } } } except BaseException : # pylint: disable=W0703 pass return toml . decoder . load ( * a , ** kw ) def main (): \"\"\"Monkey-patch `toml.load` before calling black.\"\"\" sys . argv [ 0 ] = re . sub ( r \"(-script\\.pyw?|\\.exe)?$\" , \"\" , sys . argv [ 0 ]) toml . load = patched_load sys . exit ( main_black ()) if __name__ == \"__main__\" : main ()","title":"Patch toml loader to allow .ini"},{"location":"gists/blackini/#call-blacks-main","text":"Finally, just call black: #!/usr/bin/env python # -*- coding: utf-8 -*- \"\"\"Monkeypatch black to allow reding config from '.ini` files.\"\"\" import configparser import re import sys import toml from black import main as main_black def patched_load ( * a , ** kw ): \"\"\"Attempt to read configuration from .ini, if it makes sense.\"\"\" try : from pathlib import Path file_ = a [ 0 ] if Path ( file_ ) . suffix == \".ini\" : config = configparser . ConfigParser () config . read ( file_ ) return { \"tool\" : { \"black\" : { k : v if \",\" not in v else [ p for p in v . split ( \",\" ) if p ] for k , v in config [ \"tool.black\" ] . items () } } } except BaseException : # pylint: disable=W0703 pass return toml . decoder . load ( * a , ** kw ) def main (): \"\"\"Monkey-patch `toml.load` before calling black.\"\"\" sys . argv [ 0 ] = re . sub ( r \"(-script\\.pyw?|\\.exe)?$\" , \"\" , sys . argv [ 0 ]) toml . load = patched_load sys . exit ( main_black ()) if __name__ == \"__main__\" : main () Et voil\u00e0, you have a black callable which loads config form tox.ini.","title":"Call black's main"},{"location":"gists/blackini/#closing","text":"blackini is available on PyPI here , and it has a loose dependency on black , so you can just pip install blackini (or pin a specific black version, by installing it before blackini ). Once installed, it will overwrite the black executable. This is enough for my most common use case scenario, and allows to move the black configuration inside the same tox.ini file, just like the rest of the tools.","title":"Closing"},{"location":"gists/coverage-py-editable/","text":"I was recently having some issues in a CI pipeline where the coverage report was being sent empty to codecov.io. It took me a while to figure it out, so here's the TL;DR coverage.py won't gather tests when testing a non-editable installed poetry by default installs in editable mode, which is why I had a hard time reproducing the issues locally. I was using pip install .[test] in the CI pipeline, which installs in non-editable mode. pip does not (yet) allow editable installs for pyproject.toml current solution: Use poetry in the CI pipeline to install project.","title":"Coverage.py does not like non-editable installs"},{"location":"gists/dash-or_underscores/","text":"This is a short one Variable names or any string, for that matter, when they contain a dash, can be split or navigated by their parts either by double clicking, where you'll get a partial selection or by moving with ++ctrl+arrow-right++ or by moving with ++ctrl+arrow-left++ where you'll move the cursor to the next part! When they contain an underscore instead, wont be split or navigated by their parts neither by double clicking, where you'll get a full word selection nor by moving with ++ctrl+arrow-right++ nor by moving with ++ctrl+arrow-left++ where you'll move the cursor to the next word! unless you're in `bash`, because `readline`, because `.xinputrc` For python scripts filenames both are fine, but for modules underscore is required pre-commit is a single concept, but slected/navigated as two. my_children are two words, but selected/navigated as one.","title":"Dash or Underscores?"},{"location":"gists/dash-or_underscores/#this-is-a-short-one","text":"Variable names or any string, for that matter, when they contain a dash, can be split or navigated by their parts either by double clicking, where you'll get a partial selection or by moving with ++ctrl+arrow-right++ or by moving with ++ctrl+arrow-left++ where you'll move the cursor to the next part! When they contain an underscore instead, wont be split or navigated by their parts neither by double clicking, where you'll get a full word selection nor by moving with ++ctrl+arrow-right++ nor by moving with ++ctrl+arrow-left++ where you'll move the cursor to the next word! unless you're in `bash`, because `readline`, because `.xinputrc` For python scripts filenames both are fine, but for modules underscore is required pre-commit is a single concept, but slected/navigated as two. my_children are two words, but selected/navigated as one.","title":"This is a short one"},{"location":"gists/drytoml/","text":"Drytoml Tldr Just install with pip pip install drytoml Successfully installed drytoml # contents of pyproject.dry.toml ... [tool.black] __extends = \"../../common/black.toml\" target-version = ['py37'] include = '\\.pyi?$' ... # contents of ../../common/black.toml [tool.black] line-length = 100 dry export --file=pyproject.dry.toml > pyproject.toml # contents of pyproject.toml [tool.black] line-length = 100 target-version = ['py37'] include = '\\.pyi?$' See the official documentation and usage at the official docs Motivation I have several developer tools, each with its own configuration. For formatting, there's black , isort , docformatter-toml , then, for linting, thanfully theres flakehell (actually, I required a fork, published as flakeheaven ) Gone are the days where the mere presence of a pyproject.toml would cause pip to fail installing a package, even with a healthy setup.py . Tools like blackini (RIP) have become obsolete, and there's a new sheriff in town: drytoml - which is the same as blackini , but reversed. Since all of the mentioned tools (and everyday more) have native support for pyproject.toml -based configurations, that has become my standard. However, I like to keep my configurations DRY and manageable, which is why I created drytoml: a tool which lets you reference another .toml or parts of it to configure yout tools. Another tool? what about flakehell, or nitpick? flakehell is a great tool, and it actually inspired me to extend its functionality outside of the \"lint\" scope to a \"vonfigure-all\" scope. Kinda like .editorconfig , but for the devtools. nitpick is a checker, at least for the moment. How does it work? There are two workflows: you can transclude your \"source\" into an output file, like in the example above, or you can run the included wrappers (see the official documentation for details.) The first mode is straightforward: Read the source document Each time the special codeword ( __extends by default) is found as a key in the source document, recursively merge its respective value with the contents of the referenced section On the other hand, the wrappers have extra work to do: After the previous two steps, create a temporary file (in pwd to ensure) relative paths are not broken) Configure a tool (eg black ) to use said temporary file as its configuration file. This enables to have a single source of truth in a repo. For example, a pyproject.base.toml with references on its own sections, like [tool.flakehell] , to another file in the same repo, flakehell.toml , which, in turn, contains whatever the most up-to-date nitpicky configuration I happen to have at the moment of running. Got a pylint false positive issue fixed? Time to re-enable that fix in the upstream repo, then run dry cache clear to make drytoml forget about the current (now outdated) version. Continuous integration pipelines usually are stateless, so clearing the cache is not required. Dont want to have the lates upstream version because of fear of breaking the CI pipeline? You can either use a cache for persistence, or use drytoml 's export command instead of the wrappers. I'm currently using drytoml in all my ci pipelines. Closing drytoml is available on PyPI here , and it has a loose dependency on black , so you can just pip install blackini (or pin a specific black version, by installing it before blackini ). Once installed, it will overwrite the black executable. This is enough for my most common use case scenario, and allows to move the black configuration inside the same tox.ini file, just like the rest of the tools.","title":"Drytoml"},{"location":"gists/drytoml/#drytoml","text":"Tldr Just install with pip pip install drytoml Successfully installed drytoml # contents of pyproject.dry.toml ... [tool.black] __extends = \"../../common/black.toml\" target-version = ['py37'] include = '\\.pyi?$' ... # contents of ../../common/black.toml [tool.black] line-length = 100 dry export --file=pyproject.dry.toml > pyproject.toml # contents of pyproject.toml [tool.black] line-length = 100 target-version = ['py37'] include = '\\.pyi?$' See the official documentation and usage at the official docs","title":"Drytoml"},{"location":"gists/drytoml/#motivation","text":"I have several developer tools, each with its own configuration. For formatting, there's black , isort , docformatter-toml , then, for linting, thanfully theres flakehell (actually, I required a fork, published as flakeheaven ) Gone are the days where the mere presence of a pyproject.toml would cause pip to fail installing a package, even with a healthy setup.py . Tools like blackini (RIP) have become obsolete, and there's a new sheriff in town: drytoml - which is the same as blackini , but reversed. Since all of the mentioned tools (and everyday more) have native support for pyproject.toml -based configurations, that has become my standard. However, I like to keep my configurations DRY and manageable, which is why I created drytoml: a tool which lets you reference another .toml or parts of it to configure yout tools.","title":"Motivation"},{"location":"gists/drytoml/#another-tool-what-about-flakehell-or-nitpick","text":"flakehell is a great tool, and it actually inspired me to extend its functionality outside of the \"lint\" scope to a \"vonfigure-all\" scope. Kinda like .editorconfig , but for the devtools. nitpick is a checker, at least for the moment.","title":"Another tool? what about flakehell, or nitpick?"},{"location":"gists/drytoml/#how-does-it-work","text":"There are two workflows: you can transclude your \"source\" into an output file, like in the example above, or you can run the included wrappers (see the official documentation for details.) The first mode is straightforward: Read the source document Each time the special codeword ( __extends by default) is found as a key in the source document, recursively merge its respective value with the contents of the referenced section On the other hand, the wrappers have extra work to do: After the previous two steps, create a temporary file (in pwd to ensure) relative paths are not broken) Configure a tool (eg black ) to use said temporary file as its configuration file. This enables to have a single source of truth in a repo. For example, a pyproject.base.toml with references on its own sections, like [tool.flakehell] , to another file in the same repo, flakehell.toml , which, in turn, contains whatever the most up-to-date nitpicky configuration I happen to have at the moment of running. Got a pylint false positive issue fixed? Time to re-enable that fix in the upstream repo, then run dry cache clear to make drytoml forget about the current (now outdated) version. Continuous integration pipelines usually are stateless, so clearing the cache is not required. Dont want to have the lates upstream version because of fear of breaking the CI pipeline? You can either use a cache for persistence, or use drytoml 's export command instead of the wrappers. I'm currently using drytoml in all my ci pipelines.","title":"How does it work?"},{"location":"gists/drytoml/#closing","text":"drytoml is available on PyPI here , and it has a loose dependency on black , so you can just pip install blackini (or pin a specific black version, by installing it before blackini ). Once installed, it will overwrite the black executable. This is enough for my most common use case scenario, and allows to move the black configuration inside the same tox.ini file, just like the rest of the tools.","title":"Closing"},{"location":"gists/empty-folders/","text":"TL;DR Just add this to folder-to-keep-empty/.gitignore : # .gitignore # Ignore everything in this directory * # Except this file !.gitignore Motivation This is a .gitignore which allows a folder to be committed, but ignoring all of its contents. Say you have a repo or application which requires a folder to be present, and you would like to store files inside it, without accidentally committing them. For example a logs or a data folder. Of course you could just check/create the folder on every run, but I'd like to avoid it if possible. Besides, it's nice to have a fixed tree right from the beginning (Specially during the scaffolding stages). Another advantage is you can eventually \"un-ignore\" a specific file inside the folder and keep it under version control. In the folder you'd like to remain empty, you must add a .gitignore file with a glob (*) and an exclude (!) for the file itself: mkdir -p logs curl -L git.io/commit-empty-folders > logs/.gitignore The resulting file: * !.gitignore !keep_file_in_vcs.ini References https://stackoverflow.com/questions/115983/how-can-i-add-an-empty-directory-to-a-git-repository","title":"Commiting Empty Folders"},{"location":"gists/empty-folders/#motivation","text":"This is a .gitignore which allows a folder to be committed, but ignoring all of its contents. Say you have a repo or application which requires a folder to be present, and you would like to store files inside it, without accidentally committing them. For example a logs or a data folder. Of course you could just check/create the folder on every run, but I'd like to avoid it if possible. Besides, it's nice to have a fixed tree right from the beginning (Specially during the scaffolding stages). Another advantage is you can eventually \"un-ignore\" a specific file inside the folder and keep it under version control. In the folder you'd like to remain empty, you must add a .gitignore file with a glob (*) and an exclude (!) for the file itself: mkdir -p logs curl -L git.io/commit-empty-folders > logs/.gitignore The resulting file: * !.gitignore !keep_file_in_vcs.ini References https://stackoverflow.com/questions/115983/how-can-i-add-an-empty-directory-to-a-git-repository","title":"Motivation"},{"location":"gists/github-pages-website/","text":"TL;DR gith clone https://github.com:pwoolvett/pwoolvett.github.io Motivation These are instructions for a free (:beer: and speech) webpage, hosted in github pages. Instructions Create github repo Create a repo. Go to settings (gear button, top right tab). About \u00be down the page, there's a \"Github Pages\" section. Select a source: either a full breanch, or a specific folder where docs (static pages in our case) will be located. Github pages setup For profile pages, this will be automatically enabled and fixed to master branch. For project pages, just create beforehand a gh-pages branch. Jekyll The intended way is to use jekyll. Just ignore that. We'll use mkdocs, because reasons. [OPTIONAL] Setup custom webpage Go to www.freenom.com, register and get a free (:beer:) .tk , .ml , .ga , .cf , or .gq domain. Point dns to github: After getting your own domain, go to \"Manage Freenom DNS\" and create A records with host=@ , pointing to: 185.199.1xx.153 , replacing xx with numbers 08 , 09 , 10 , and 11 (four records in total). Using mkdocs to create html from markdown We'll create this folder structure: . \u251c\u2500\u2500 docker-compose.yml \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 CNAME \u2502 \u251c\u2500\u2500 logo.png \u2502 \u2514\u2500\u2500 index.md \u251c\u2500\u2500 entrypoint.sh \u2514\u2500\u2500 mkdocs.yml ./docker-compose.yml version : '3.6' services : mkdocs : image : squidfunk/mkdocs-material ports : - \"8000:8000\" volumes : - ./:/docs - ~/.ssh:/tmp/.ssh:ro - ~/.gitconfig:/root/.gitconfig:ro - ./entrypoint.sh:/usr/local/bin/entrypoint:ro SSH The last three volumes are only used if you want to use github ssh remote. For https its not required. master branch If you're using the master branch (for your personal page) and want to run mkdocs gh-deploy (the \"deploy to github\" command), also make sure to pass -b master . docs/CNAME Just an empty text file with the domain you acquired and nothing else, eg: echo mywebsite.gq > docs/CNAME docs/index.md All the other \"source\" files can be named whatever-you-want.md . Just name the first index.md and put whatever markdown makes sense for you. ./entrypoint.sh Only required if using ssh remote. This is just a workaround to allow the default docker user to use a previously configured ssh credentials without permission issues: set -e cp -R /tmp/.ssh /root/.ssh chmod 700 /root/.ssh chmod 644 /root/.ssh/id_rsa.pub chmod 600 /root/.ssh/id_rsa exec \" $@ \" ./mkdocs.yml site_name : <<Your site name here>> theme : name : 'material' palette : primary : 'white' accent : 'indigo' logo : '<<Your logo here>>' favicon : '<<Your logo here>>' markdown_extensions : - admonition - pymdownx.highlight - toc : permalink : true - footnotes - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji : emoji_generator : !!python/name:pymdownx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde - meta extra_javascript : - 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML' - https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js # - javascripts/config.js - js/termynal.js - js/customTermynals.js extra_css : - https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css - css/termynal.css nav : - Home : index.md other files get termynal js and css , put them inside docs/js and docs/css , respectively. To automatically init termynals, put this in docs/js/customTermynals.js : function initTermynals (){ document . querySelectorAll ( '[id^=\"termynal\"]' ). forEach ( terminal =>{ new Termynal ( terminal ); } ); } initTermynals (); [OPTIONAL] Checking before deploying Although not necessary, you can preview the output you'll get either by running mkdocs serve , which serves and auto-reloads the html form your markdown, or mkdocs build . [OPTIONAL] Push/deploy webpage When you're happy with your results, run mkdocs gh-deploy to push your changes to github. After a short while, your new content will be available in your webpage. Sources [Github Pages] (https://pages.github.com/) [mkdocs] (https://www.mkdocs.org/) [mkdocs-material] (https://github.com/squidfunk/mkdocs-material) [this very page] (https://github.com/pwoolvett/pwoolvett.github.io)","title":"Github Pages Website"},{"location":"gists/github-pages-website/#motivation","text":"These are instructions for a free (:beer: and speech) webpage, hosted in github pages.","title":"Motivation"},{"location":"gists/github-pages-website/#instructions","text":"","title":"Instructions"},{"location":"gists/github-pages-website/#create-github-repo","text":"Create a repo. Go to settings (gear button, top right tab). About \u00be down the page, there's a \"Github Pages\" section. Select a source: either a full breanch, or a specific folder where docs (static pages in our case) will be located. Github pages setup For profile pages, this will be automatically enabled and fixed to master branch. For project pages, just create beforehand a gh-pages branch. Jekyll The intended way is to use jekyll. Just ignore that. We'll use mkdocs, because reasons.","title":"Create github repo"},{"location":"gists/github-pages-website/#optional-setup-custom-webpage","text":"Go to www.freenom.com, register and get a free (:beer:) .tk , .ml , .ga , .cf , or .gq domain. Point dns to github: After getting your own domain, go to \"Manage Freenom DNS\" and create A records with host=@ , pointing to: 185.199.1xx.153 , replacing xx with numbers 08 , 09 , 10 , and 11 (four records in total).","title":"[OPTIONAL] Setup custom webpage"},{"location":"gists/github-pages-website/#using-mkdocs-to-create-html-from-markdown","text":"We'll create this folder structure: . \u251c\u2500\u2500 docker-compose.yml \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 CNAME \u2502 \u251c\u2500\u2500 logo.png \u2502 \u2514\u2500\u2500 index.md \u251c\u2500\u2500 entrypoint.sh \u2514\u2500\u2500 mkdocs.yml","title":"Using mkdocs to create html from markdown"},{"location":"gists/github-pages-website/#docker-composeyml","text":"version : '3.6' services : mkdocs : image : squidfunk/mkdocs-material ports : - \"8000:8000\" volumes : - ./:/docs - ~/.ssh:/tmp/.ssh:ro - ~/.gitconfig:/root/.gitconfig:ro - ./entrypoint.sh:/usr/local/bin/entrypoint:ro SSH The last three volumes are only used if you want to use github ssh remote. For https its not required. master branch If you're using the master branch (for your personal page) and want to run mkdocs gh-deploy (the \"deploy to github\" command), also make sure to pass -b master .","title":"./docker-compose.yml"},{"location":"gists/github-pages-website/#docscname","text":"Just an empty text file with the domain you acquired and nothing else, eg: echo mywebsite.gq > docs/CNAME","title":"docs/CNAME"},{"location":"gists/github-pages-website/#docsindexmd","text":"All the other \"source\" files can be named whatever-you-want.md . Just name the first index.md and put whatever markdown makes sense for you.","title":"docs/index.md"},{"location":"gists/github-pages-website/#entrypointsh","text":"Only required if using ssh remote. This is just a workaround to allow the default docker user to use a previously configured ssh credentials without permission issues: set -e cp -R /tmp/.ssh /root/.ssh chmod 700 /root/.ssh chmod 644 /root/.ssh/id_rsa.pub chmod 600 /root/.ssh/id_rsa exec \" $@ \"","title":"./entrypoint.sh"},{"location":"gists/github-pages-website/#mkdocsyml","text":"site_name : <<Your site name here>> theme : name : 'material' palette : primary : 'white' accent : 'indigo' logo : '<<Your logo here>>' favicon : '<<Your logo here>>' markdown_extensions : - admonition - pymdownx.highlight - toc : permalink : true - footnotes - pymdownx.arithmatex - pymdownx.betterem : smart_enable : all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji : emoji_generator : !!python/name:pymdownx.emoji.to_svg - pymdownx.inlinehilite - pymdownx.magiclink - pymdownx.mark - pymdownx.smartsymbols - pymdownx.superfences - pymdownx.tasklist : custom_checkbox : true - pymdownx.tilde - meta extra_javascript : - 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML' - https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js # - javascripts/config.js - js/termynal.js - js/customTermynals.js extra_css : - https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css - css/termynal.css nav : - Home : index.md","title":"./mkdocs.yml"},{"location":"gists/github-pages-website/#other-files","text":"get termynal js and css , put them inside docs/js and docs/css , respectively. To automatically init termynals, put this in docs/js/customTermynals.js : function initTermynals (){ document . querySelectorAll ( '[id^=\"termynal\"]' ). forEach ( terminal =>{ new Termynal ( terminal ); } ); } initTermynals ();","title":"other files"},{"location":"gists/github-pages-website/#optional-checking-before-deploying","text":"Although not necessary, you can preview the output you'll get either by running mkdocs serve , which serves and auto-reloads the html form your markdown, or mkdocs build .","title":"[OPTIONAL] Checking before deploying"},{"location":"gists/github-pages-website/#optional-pushdeploy-webpage","text":"When you're happy with your results, run mkdocs gh-deploy to push your changes to github. After a short while, your new content will be available in your webpage. Sources [Github Pages] (https://pages.github.com/) [mkdocs] (https://www.mkdocs.org/) [mkdocs-material] (https://github.com/squidfunk/mkdocs-material) [this very page] (https://github.com/pwoolvett/pwoolvett.github.io)","title":"[OPTIONAL] Push/deploy webpage"},{"location":"gists/gstlaunch-bins/","text":"TL;DR export caps = 'video/x-raw(memory:NVMM),width=1280,height=720' export DISPLAY = :0 gst-launch-1.0 bin \\ \\( videotestsrc pattern = snow ! videorate ! nvvideoconvert ! capsfilter caps = $caps \\) \\ ! mux.sink_0 nvstreammux name = mux width = 1920 height = 1080 batch-size = 1 \\ ! nvvideoconvert \\ ! videoconvert \\ ! xvimagesink \\ bin \\ \\( videotestsrc pattern = ball ! videorate ! nvvideoconvert ! capsfilter caps = $caps \\) \\ ! mux.sink_1 Motivation When woking with Gstreamer, a useful abstraction is the bin element, which can be used to encapsulate sub-sections of a pipeline, like a black box. When developing a GStreamer-based application, it is sometimes useful to isolate the gstreamer pipeine itself from the rest of the application logic (eg for performance optimization). It happened to me that each time I wanted to ask a question in a forum, I had to simplify the Pipeline and reduce it to a minimal example so as to concentrate in the specific question at hand. It was during this stage that we developed a small procedure to check the pipeline's behavior: just run it using gst-launch . Problem is, I was dynamically building the pipeline, and it used bins to achieve this blackbox effect on the source... Which is why the command at the beginning of the post is useful: as a template to reconstruct more complex pipelines which use bins and be able to run them with the gst-launch utility.","title":"Using bin elements in Gstreamer from gstlaunch"},{"location":"gists/gstlaunch-bins/#motivation","text":"When woking with Gstreamer, a useful abstraction is the bin element, which can be used to encapsulate sub-sections of a pipeline, like a black box. When developing a GStreamer-based application, it is sometimes useful to isolate the gstreamer pipeine itself from the rest of the application logic (eg for performance optimization). It happened to me that each time I wanted to ask a question in a forum, I had to simplify the Pipeline and reduce it to a minimal example so as to concentrate in the specific question at hand. It was during this stage that we developed a small procedure to check the pipeline's behavior: just run it using gst-launch . Problem is, I was dynamically building the pipeline, and it used bins to achieve this blackbox effect on the source... Which is why the command at the beginning of the post is useful: as a template to reconstruct more complex pipelines which use bins and be able to run them with the gst-launch utility.","title":"Motivation"},{"location":"gists/integrating-behave-into-pytest/","text":"Motivation I genreally use two testing frameworks for python: pytest for unit and integration testing, and behave for bdd . Depending on what role i take in the project, I tend to suggest / enforce one of them. However, I recently found myself requiring a combination of the two, as I was supervising a project development (thus behave), bus also tracking down an issue produced by a combination of edge cases. Because of this, I started my quest for the holy grial: running behave from within pytest. TLDR There was an unmaintained, broken project , and another framework for python+bdd, which could be run directly from pytest! Naturally, I did what any sane person would do: consider implementing the integration yourself and tollay disregard the alternatives (: Sample implementation There's an example for a yaml checker in the official pytest docs. The only thing I had to do was copy and paste, replacing .yaml for .feature . # content of conftest.py import pytest def pytest_collect_file ( parent , path ): \"\"\"Allow .feature files to be parsed for bdd.\"\"\" if path . ext == \".feature\" : return BehaveFile . from_parent ( parent , fspath = path ) class BehaveFile ( pytest . File ): \"\"\"A .feature file which yields all of its scenarios/outlines.\"\"\" def collect ( self ): from behave.parser import parse_file feature = parse_file ( self . fspath ) for scenario in feature . walk_scenarios ( with_outlines = True ): yield BehaveFeature . from_parent ( self , name = scenario . name , feature = feature , scenario = scenario , ) class BehaveFeature ( pytest . Item ): def __init__ ( self , name , parent , feature , scenario ): super () . __init__ ( name , parent ) self . _feature = feature self . _scenario = scenario def runtest ( self ): \"\"\"Wrapper implementation which calls behave as a subprocess.\"\"\" import subprocess as sp from shlex import split feature_name = self . _feature . filename cmd = split ( f \"\"\"behave tests/bdd/ --format json --no-summary --include { feature_name } -n \" { self . _scenario . name } \" \"\"\" ) try : proc = sp . run ( cmd , stdout = sp . PIPE ) if not proc . returncode : return except Exception as exc : raise BehaveException ( self , f \"exc= { exc } , feature= { feature_name } \" ) stdout = proc . stdout . decode ( \"utf8\" ) raise BehaveException ( self , stdout ) def repr_failure ( self , excinfo ): \"\"\"Called when self.runtest() raises an exception.\"\"\" import json if isinstance ( excinfo . value , BehaveException ): feature = excinfo . value . args [ 0 ] . _feature results = excinfo . value . args [ 1 ] data = json . loads ( results ) summary = \"\" for feature in data : if feature [ 'status' ] != \"failed\" : continue summary += f \" \\n Feature: { feature [ 'name' ] } \" for element in feature [ \"elements\" ]: if element [ 'status' ] != \"failed\" : continue summary += f \" \\n { element [ 'type' ] . title () } : { element [ 'name' ] } \" for step in element [ \"steps\" ]: try : result = step [ 'result' ] except KeyError : summary += f \" \\n Step [NOT REACHED]: { step [ 'name' ] } \" continue status = result [ 'status' ] if status != \"failed\" : summary += f \" \\n Step [OK]: { step [ 'name' ] } \" else : summary += f \" \\n Step [ERR]: { step [ 'name' ] } \" summary += \" \\n \" + \" \\n \" . join ( result [ 'error_message' ]) return summary def reportinfo ( self ): return self . fspath , 0 , f \"Feature: { self . _feature . name } - Scenario: { self . _scenario . name } \" Info https://stackoverflow.com/a/66284525/7814595","title":"Integrating behave into pytest"},{"location":"gists/integrating-behave-into-pytest/#motivation","text":"I genreally use two testing frameworks for python: pytest for unit and integration testing, and behave for bdd . Depending on what role i take in the project, I tend to suggest / enforce one of them. However, I recently found myself requiring a combination of the two, as I was supervising a project development (thus behave), bus also tracking down an issue produced by a combination of edge cases. Because of this, I started my quest for the holy grial: running behave from within pytest.","title":"Motivation"},{"location":"gists/integrating-behave-into-pytest/#tldr","text":"There was an unmaintained, broken project , and another framework for python+bdd, which could be run directly from pytest! Naturally, I did what any sane person would do: consider implementing the integration yourself and tollay disregard the alternatives (:","title":"TLDR"},{"location":"gists/integrating-behave-into-pytest/#sample-implementation","text":"There's an example for a yaml checker in the official pytest docs. The only thing I had to do was copy and paste, replacing .yaml for .feature . # content of conftest.py import pytest def pytest_collect_file ( parent , path ): \"\"\"Allow .feature files to be parsed for bdd.\"\"\" if path . ext == \".feature\" : return BehaveFile . from_parent ( parent , fspath = path ) class BehaveFile ( pytest . File ): \"\"\"A .feature file which yields all of its scenarios/outlines.\"\"\" def collect ( self ): from behave.parser import parse_file feature = parse_file ( self . fspath ) for scenario in feature . walk_scenarios ( with_outlines = True ): yield BehaveFeature . from_parent ( self , name = scenario . name , feature = feature , scenario = scenario , ) class BehaveFeature ( pytest . Item ): def __init__ ( self , name , parent , feature , scenario ): super () . __init__ ( name , parent ) self . _feature = feature self . _scenario = scenario def runtest ( self ): \"\"\"Wrapper implementation which calls behave as a subprocess.\"\"\" import subprocess as sp from shlex import split feature_name = self . _feature . filename cmd = split ( f \"\"\"behave tests/bdd/ --format json --no-summary --include { feature_name } -n \" { self . _scenario . name } \" \"\"\" ) try : proc = sp . run ( cmd , stdout = sp . PIPE ) if not proc . returncode : return except Exception as exc : raise BehaveException ( self , f \"exc= { exc } , feature= { feature_name } \" ) stdout = proc . stdout . decode ( \"utf8\" ) raise BehaveException ( self , stdout ) def repr_failure ( self , excinfo ): \"\"\"Called when self.runtest() raises an exception.\"\"\" import json if isinstance ( excinfo . value , BehaveException ): feature = excinfo . value . args [ 0 ] . _feature results = excinfo . value . args [ 1 ] data = json . loads ( results ) summary = \"\" for feature in data : if feature [ 'status' ] != \"failed\" : continue summary += f \" \\n Feature: { feature [ 'name' ] } \" for element in feature [ \"elements\" ]: if element [ 'status' ] != \"failed\" : continue summary += f \" \\n { element [ 'type' ] . title () } : { element [ 'name' ] } \" for step in element [ \"steps\" ]: try : result = step [ 'result' ] except KeyError : summary += f \" \\n Step [NOT REACHED]: { step [ 'name' ] } \" continue status = result [ 'status' ] if status != \"failed\" : summary += f \" \\n Step [OK]: { step [ 'name' ] } \" else : summary += f \" \\n Step [ERR]: { step [ 'name' ] } \" summary += \" \\n \" + \" \\n \" . join ( result [ 'error_message' ]) return summary def reportinfo ( self ): return self . fspath , 0 , f \"Feature: { self . _feature . name } - Scenario: { self . _scenario . name } \" Info https://stackoverflow.com/a/66284525/7814595","title":"Sample implementation"},{"location":"tutorials/in-the-beginning/","text":"Must read articles Read these before everything else, and keep them in mind when developing / contributing: Bikeshed, or \"the sleep(1) saga\" PEP 8 Conventional Commits semver Don't worry too much about the specifics, but grasp the general idea behind them. Principles On linters Use the lint checkers as suggestions, not rules. Dont go on a lint frenzy just to have 0 lint warnings. For example, check the following snippet: def complex_function ( value , previous , next , state , slope , alpha , n_calls ): ... A linter like pylint will raise a R0913 , suggesting you have too many arguments. To overcome this, you could just put everything inside a dictionary, or pass *args, **kwargs to silence the warning. That's not why we use linters. Instead, ask yourself what should be done with the code, and if it makes sense to even change it. Maybe the current implementation is the only (or the best) way to implement it. The reference mentions the following: When a function or method takes a lot of arguments, it is hard to understand and remember what their purpose is. If the arguments are closely related, maybe there is a new class that groups them waiting to be implemented. Just don't run into conclusions yet. Think also on the rest of the code. Keep in mind any implications, breaking changes this would incur, etc.","title":"Must read articles"},{"location":"tutorials/in-the-beginning/#must-read-articles","text":"Read these before everything else, and keep them in mind when developing / contributing: Bikeshed, or \"the sleep(1) saga\" PEP 8 Conventional Commits semver Don't worry too much about the specifics, but grasp the general idea behind them.","title":"Must read articles"},{"location":"tutorials/in-the-beginning/#principles","text":"","title":"Principles"},{"location":"tutorials/in-the-beginning/#on-linters","text":"Use the lint checkers as suggestions, not rules. Dont go on a lint frenzy just to have 0 lint warnings. For example, check the following snippet: def complex_function ( value , previous , next , state , slope , alpha , n_calls ): ... A linter like pylint will raise a R0913 , suggesting you have too many arguments. To overcome this, you could just put everything inside a dictionary, or pass *args, **kwargs to silence the warning. That's not why we use linters. Instead, ask yourself what should be done with the code, and if it makes sense to even change it. Maybe the current implementation is the only (or the best) way to implement it. The reference mentions the following: When a function or method takes a lot of arguments, it is hard to understand and remember what their purpose is. If the arguments are closely related, maybe there is a new class that groups them waiting to be implemented. Just don't run into conclusions yet. Think also on the rest of the code. Keep in mind any implications, breaking changes this would incur, etc.","title":"On linters"},{"location":"tutorials/python-setup-v2/","text":"Topics covered TL;WR Python installation Virtual environments Installing libraries with pip Project Structure Python setup tutorial Python Version management pyenv: THE python version manager Do not install python on your own. Instead, use a version manager, like pyenv : curl https://pyenv.run | bash Build python prerequisites: Before bein able to build python using pyenv, you'll probably need a bunch of tools ( See here for a reference and instructions for other OS ): Ubuntu/Debian sudo apt-get install -y \\ build-essential \\ libssl-dev \\ zlib1g-dev \\ libbz2-dev \\ libreadline-dev \\ libsqlite3-dev \\ wget \\ curl \\ llvm \\ libncurses5-dev \\ libncursesw5-dev \\ xz-utils \\ tk-dev \\ libffi-dev \\ liblzma-dev \\ python-openssl \\ git Alpine apk add --no-cache \\ bzip2-dev \\ coreutils \\ dpkg-dev \\ dpkg \\ expat-dev \\ findutils \\ gcc \\ gdbm-dev \\ libc-dev \\ libffi-dev \\ libnsl-dev \\ libtirpc-dev \\ linux-headers \\ make \\ ncurses-dev \\ openssl-dev \\ pax-utils \\ readline-dev \\ sqlite-dev \\ tcl-dev \\ tk \\ tk-dev \\ util-linux-dev \\ xz-dev \\ zlib-dev # And to use pyenv-installer apk add \\ git \\ curl \\ bash Arch pacman -S --needed \\ base-devel \\ openssl \\ zlib \\ bzip2 \\ readline \\ sqlite \\ curl \\ llvm \\ ncurses \\ xz \\ tk \\ libffi \\ python-pyopenssl \\ git # And for ncurses5 yay -S ncurses5-compat-libs Installing python versions Use pyenv to install python versions. pyenv isntall --list will show available versions: dev and stable, old and new. Even those outside the default implementation, like pypy, jython, micropython, etc. So, you know... a lot of versions Pick one and install: pyenv install 3.9.0 Setting a custom interpreter To set a custom python as THE python interprerer: pyenv local 3.9.0 This creates a .python-version file which tells the pyenv prompt command to ensure that specific python is the first one found in $PATH. Managing Virtual Environments Your system probably has a python. Maybe two versions. Avoid using them or installing stuff using those pip . Instead, for each project you should use a different virtual environment. This avoids having dependency issues and makes it easier to reproduce. The only exception There are some tools which should be installed on your system. The only one I recommend is pipx python3 - m pip install -- user pipx now pypi is an application store! Install cross-project tools using pipx This ensures every executable has its own, isolated virtualenv. Go ahead and install poetry with this method. pipx install poetry Whatever you find on pypi can be installed this way, so you could, for example: pipx install docker - compose or pipx install youtube - dl etc. Actually Managing Venvs python comes with a venv module there's a virtualenv package pipx creates a venv for every 'app' pyenv can manage virtual envs poetry can manage virtual envs pipenv (similar to poetry) anaconda and friends also manage venvs Also, there's docker, virtual machines, tox, ... It does not matter how or which tool you use, just use virtual envs. Also, use poetry :wink:. So, you have a project which requires python3.7 : pyenv install 3.7.x # this is required once, after this, that version will ve available to use in your system. # instead of x, press TAB and vhoose tha latest available pyenv local 3.7.x # ensure while in this directory that is what \"python\" is bound to poetry [install|shell|new|init|env] # the corresponding command for the project stage youre at you can liat them with poetry env --list","title":"Snakes and Ladders II - Snakes Reloaded"},{"location":"tutorials/python-setup-v2/#topics-covered","text":"TL;WR Python installation Virtual environments Installing libraries with pip Project Structure","title":"Topics covered"},{"location":"tutorials/python-setup-v2/#python-setup-tutorial","text":"","title":"Python setup tutorial"},{"location":"tutorials/python-setup-v2/#python-version-management","text":"","title":"Python Version management"},{"location":"tutorials/python-setup-v2/#pyenv-the-python-version-manager","text":"Do not install python on your own. Instead, use a version manager, like pyenv : curl https://pyenv.run | bash","title":"pyenv: THE python version manager"},{"location":"tutorials/python-setup-v2/#build-python-prerequisites","text":"Before bein able to build python using pyenv, you'll probably need a bunch of tools ( See here for a reference and instructions for other OS ): Ubuntu/Debian sudo apt-get install -y \\ build-essential \\ libssl-dev \\ zlib1g-dev \\ libbz2-dev \\ libreadline-dev \\ libsqlite3-dev \\ wget \\ curl \\ llvm \\ libncurses5-dev \\ libncursesw5-dev \\ xz-utils \\ tk-dev \\ libffi-dev \\ liblzma-dev \\ python-openssl \\ git Alpine apk add --no-cache \\ bzip2-dev \\ coreutils \\ dpkg-dev \\ dpkg \\ expat-dev \\ findutils \\ gcc \\ gdbm-dev \\ libc-dev \\ libffi-dev \\ libnsl-dev \\ libtirpc-dev \\ linux-headers \\ make \\ ncurses-dev \\ openssl-dev \\ pax-utils \\ readline-dev \\ sqlite-dev \\ tcl-dev \\ tk \\ tk-dev \\ util-linux-dev \\ xz-dev \\ zlib-dev # And to use pyenv-installer apk add \\ git \\ curl \\ bash Arch pacman -S --needed \\ base-devel \\ openssl \\ zlib \\ bzip2 \\ readline \\ sqlite \\ curl \\ llvm \\ ncurses \\ xz \\ tk \\ libffi \\ python-pyopenssl \\ git # And for ncurses5 yay -S ncurses5-compat-libs","title":"Build python prerequisites:"},{"location":"tutorials/python-setup-v2/#installing-python-versions","text":"Use pyenv to install python versions. pyenv isntall --list will show available versions: dev and stable, old and new. Even those outside the default implementation, like pypy, jython, micropython, etc. So, you know... a lot of versions Pick one and install: pyenv install 3.9.0","title":"Installing python versions"},{"location":"tutorials/python-setup-v2/#setting-a-custom-interpreter","text":"To set a custom python as THE python interprerer: pyenv local 3.9.0 This creates a .python-version file which tells the pyenv prompt command to ensure that specific python is the first one found in $PATH.","title":"Setting a custom interpreter"},{"location":"tutorials/python-setup-v2/#managing-virtual-environments","text":"Your system probably has a python. Maybe two versions. Avoid using them or installing stuff using those pip . Instead, for each project you should use a different virtual environment. This avoids having dependency issues and makes it easier to reproduce.","title":"Managing Virtual Environments"},{"location":"tutorials/python-setup-v2/#the-only-exception","text":"There are some tools which should be installed on your system. The only one I recommend is pipx python3 - m pip install -- user pipx now pypi is an application store!","title":"The only exception"},{"location":"tutorials/python-setup-v2/#install-cross-project-tools-using-pipx","text":"This ensures every executable has its own, isolated virtualenv. Go ahead and install poetry with this method. pipx install poetry Whatever you find on pypi can be installed this way, so you could, for example: pipx install docker - compose or pipx install youtube - dl etc.","title":"Install cross-project tools using pipx"},{"location":"tutorials/python-setup-v2/#actually-managing-venvs","text":"python comes with a venv module there's a virtualenv package pipx creates a venv for every 'app' pyenv can manage virtual envs poetry can manage virtual envs pipenv (similar to poetry) anaconda and friends also manage venvs Also, there's docker, virtual machines, tox, ... It does not matter how or which tool you use, just use virtual envs. Also, use poetry :wink:. So, you have a project which requires python3.7 : pyenv install 3.7.x # this is required once, after this, that version will ve available to use in your system. # instead of x, press TAB and vhoose tha latest available pyenv local 3.7.x # ensure while in this directory that is what \"python\" is bound to poetry [install|shell|new|init|env] # the corresponding command for the project stage youre at you can liat them with poetry env --list","title":"Actually Managing Venvs"},{"location":"tutorials/python-setup/","text":"Attention This post is replaced by the next one in the series. Its left for reference only. python yaay This is the first of several posts about common python stuff, starting from installation, through configuration of a project, and explaining an opinionated (and maybe over-complicated) python setup. As a whole, they explain my road to to My Template Topics covered TL;WR Python installation Virtual environments Installing libraries with pip Project Structure Too long, Won't Read Install python: sudo apt install python3.7 Install virtualenv lib: sudo apt install python3.7-venv Create project: cd ~ mkdir project_a cd project_a touch README echo \"print('hola mundo')\" > script_a.py python3.7 -m venv .venv Activate virtualenv: source .venv/bin/activate install and declare libs: pip install antigravity pip freeze > requirements.txt Run a script: python script_a.py That's it. Next sections talk about what, how, and why these commands make sense to me. I. Python installation picture alt You will want to install / download python, regardless if you and/or your os already have/has one. Go to Download Python Click on \"download Python 3.x.y\" (as of today, latest version is 3.7.3) The numbers represent MAJOR.MINOR.PATCH : MAJOR version when you make incompatible API changes MINOR version when you add functionality in a backwards-compatible manner PATCH version when you make backwards-compatible bug fixes. Which version should I install? Check any library dependencies / restrictions. If starting from scratch, start with the highest available. In the following, I'll assume you have python 3.7: sudo apt install python3.7 Now, if all went OK, you can now do this: python3.7 -c \"print('hola mundo')\" , which just runs the comand inside the \" in the python interpreter, The \u00b43.7\u00b4 might not be necessary, but if you had another python version installed, it'll make sure we're calling the right one. Virtual environments Before python, I was used to Matlab/Octave. There, when you need an extra library, you just add the \"package\" to the whole program, and that was it. For research purposes, it was more than enough. In python, the (standard) way to do this (install a library), is by invoking the ancestral magic: pip install [xxx] , where xxx is a any package available in PYPI . Don't do this yet. So now you have a project (let's call it \"project A\"). You can execute python, run scripts, install libraries, etc. Before continuing, it might be useful to ask yourself: who's the intended receipient for the code? It most likely won't be (just) you -sorry if you are-. I certainly hope it's not your OS either -really sorry if it really is-. Even if that is the case, you wouldn't want to install your packages/libraries into your system, right? ... Right? What if later, you have another project called \"project B\" (I know, very original, right?), but project B has different dependencies? Or if a library update breaks your awesome code? You want to have an environment for your project which is as reproducible and controllable as possible, Regardless of the (un)tidyness of the recipient's python interpreter. Bfe it your own OS's python or your buddy's, your bossesses' or your client's, you want a python as clean as possible, with all and only the required libraries for project A. What about project B then? Enter virtual environments python docs pep 0405 . From the docs: \"lightweight 'virtual environments' with their own site directories, optionally isolated from system site directories. Each virtual environment has its own Python binary (which matches the version of the binary that was used to create this environment) and can have its own independent set of installed Python packages in its site directories.\" This translates to: put python stuff inside a bubble specifically tailored for \"project A\". To do this, we need another python library called \u00b4venv\u00b4: python3.7 -m venv my_first_virtualenv That command creates a folder \"my_first_virtualenv\" containing all necessary stuff to run python isolated from your system, or \"project B\". Now, the last piece of the puzzle is to make sure that we use the \"new\" python instead of the system's. This is called \"activating\" the virtual environment. For example, with \u00b4source path_to_my_virtualenv/bin/activate\u00b4 (linux) or \u00b4path_to_my_virtualenv/Scripts/activate.bat\u00b4 (win). This is just a trick wich pre-pends the folders of the respective virtualenv in the \u00b4PATH\u00b4 environment variable . Now, calling \u00b4python\u00b4 will use the virtualenv executable instead of your system's. If the activation is correct, you should see a \u00b4(virtualenv_name)\u00b4 before the command prompt. In order to deactivate it, just run \u00b4deactivate\u00b4, or close the terminal (don't worry, your \u00b4PATH\u00b4 variable is only altered for the running session, inside the terminal) Once activated, you can just run python [whatever] and it'll fint the python in the activated. Try playing with which python (unix) or where python (windows) before and after activation, and see what happens. Try again with pip instead of python . Installing libraries and pip Finally, when installing libraries, we'll want to install them inside the virtualenv. So just make sure the virtualenv is activated, and perform the ritual: pip install xxx . At any moment, if you with to see installed libraries, just run pip freeze (with the venv activated). Except for pkg-resources==0.0.0 (if present, it's a known bug ), every line is a package with its respective version. Now, if you want someone else to replicate your environment, you have to tell them which libraries are required: pip freee > requirements.txt With this, you can send them your code together with the requirements file, and they can install all requirements: pip install -r requirements.txt (whithin a proper virtualenv, if they're civilized)","title":"Snakes and Ladders I - The pythoning"},{"location":"tutorials/python-setup/#topics-covered","text":"TL;WR Python installation Virtual environments Installing libraries with pip Project Structure","title":"Topics covered"},{"location":"tutorials/python-setup/#too-long-wont-read","text":"Install python: sudo apt install python3.7 Install virtualenv lib: sudo apt install python3.7-venv Create project: cd ~ mkdir project_a cd project_a touch README echo \"print('hola mundo')\" > script_a.py python3.7 -m venv .venv Activate virtualenv: source .venv/bin/activate install and declare libs: pip install antigravity pip freeze > requirements.txt Run a script: python script_a.py","title":"Too long, Won't Read"},{"location":"tutorials/python-setup/#thats-it-next-sections-talk-about-what-how-and-why-these-commands-make-sense-to-me","text":"","title":"That's it. Next sections talk about what, how, and why these commands make sense to me."},{"location":"tutorials/python-setup/#i-python-installation","text":"picture alt You will want to install / download python, regardless if you and/or your os already have/has one. Go to Download Python Click on \"download Python 3.x.y\" (as of today, latest version is 3.7.3) The numbers represent MAJOR.MINOR.PATCH : MAJOR version when you make incompatible API changes MINOR version when you add functionality in a backwards-compatible manner PATCH version when you make backwards-compatible bug fixes. Which version should I install? Check any library dependencies / restrictions. If starting from scratch, start with the highest available. In the following, I'll assume you have python 3.7: sudo apt install python3.7 Now, if all went OK, you can now do this: python3.7 -c \"print('hola mundo')\" , which just runs the comand inside the \" in the python interpreter, The \u00b43.7\u00b4 might not be necessary, but if you had another python version installed, it'll make sure we're calling the right one.","title":"I. Python installation"},{"location":"tutorials/python-setup/#virtual-environments","text":"Before python, I was used to Matlab/Octave. There, when you need an extra library, you just add the \"package\" to the whole program, and that was it. For research purposes, it was more than enough. In python, the (standard) way to do this (install a library), is by invoking the ancestral magic: pip install [xxx] , where xxx is a any package available in PYPI . Don't do this yet. So now you have a project (let's call it \"project A\"). You can execute python, run scripts, install libraries, etc. Before continuing, it might be useful to ask yourself: who's the intended receipient for the code? It most likely won't be (just) you -sorry if you are-. I certainly hope it's not your OS either -really sorry if it really is-. Even if that is the case, you wouldn't want to install your packages/libraries into your system, right? ... Right? What if later, you have another project called \"project B\" (I know, very original, right?), but project B has different dependencies? Or if a library update breaks your awesome code? You want to have an environment for your project which is as reproducible and controllable as possible, Regardless of the (un)tidyness of the recipient's python interpreter. Bfe it your own OS's python or your buddy's, your bossesses' or your client's, you want a python as clean as possible, with all and only the required libraries for project A. What about project B then? Enter virtual environments python docs pep 0405 . From the docs: \"lightweight 'virtual environments' with their own site directories, optionally isolated from system site directories. Each virtual environment has its own Python binary (which matches the version of the binary that was used to create this environment) and can have its own independent set of installed Python packages in its site directories.\" This translates to: put python stuff inside a bubble specifically tailored for \"project A\". To do this, we need another python library called \u00b4venv\u00b4: python3.7 -m venv my_first_virtualenv That command creates a folder \"my_first_virtualenv\" containing all necessary stuff to run python isolated from your system, or \"project B\". Now, the last piece of the puzzle is to make sure that we use the \"new\" python instead of the system's. This is called \"activating\" the virtual environment. For example, with \u00b4source path_to_my_virtualenv/bin/activate\u00b4 (linux) or \u00b4path_to_my_virtualenv/Scripts/activate.bat\u00b4 (win). This is just a trick wich pre-pends the folders of the respective virtualenv in the \u00b4PATH\u00b4 environment variable . Now, calling \u00b4python\u00b4 will use the virtualenv executable instead of your system's. If the activation is correct, you should see a \u00b4(virtualenv_name)\u00b4 before the command prompt. In order to deactivate it, just run \u00b4deactivate\u00b4, or close the terminal (don't worry, your \u00b4PATH\u00b4 variable is only altered for the running session, inside the terminal) Once activated, you can just run python [whatever] and it'll fint the python in the activated. Try playing with which python (unix) or where python (windows) before and after activation, and see what happens. Try again with pip instead of python .","title":"Virtual environments"},{"location":"tutorials/python-setup/#installing-libraries-and-pip","text":"Finally, when installing libraries, we'll want to install them inside the virtualenv. So just make sure the virtualenv is activated, and perform the ritual: pip install xxx . At any moment, if you with to see installed libraries, just run pip freeze (with the venv activated). Except for pkg-resources==0.0.0 (if present, it's a known bug ), every line is a package with its respective version. Now, if you want someone else to replicate your environment, you have to tell them which libraries are required: pip freee > requirements.txt With this, you can send them your code together with the requirements file, and they can install all requirements: pip install -r requirements.txt (whithin a proper virtualenv, if they're civilized)","title":"Installing libraries and pip"}]}